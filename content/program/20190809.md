---
title: "初談AI, ML & DL (1)"
date: 2019-08-09T11:40:11+08:00
publishdate: 2019-08-09T11:40:11+08:00
tags: ["deep learning", "machine learning"]
comments: false
---

### 前言
在近幾年，常常聽到業界或學界，很喜歡說，遇到困難的問題可以嘗試用AI解、產品一定要跟AI扯上關係，或是收集資料很重要啊，然後什麼未來是AI的世界阿、產品沒有AI會拿不到投資之類的(尤其非工程領域的專業人士更愛講)。
<!--more-->

然後進一步更有趣的現象是，有時候聽到有人提AI，有時候又聽到Machine Learning，有時候又聽到Deep Learning，但除了相關領域的工程人員，真正弄得清楚這之間的差別的人又有多少呢?

於是在這，就先來

- 簡單了解AI, Machine Learning, Deep Learning有什麼差別吧!
- 實作上的一些基本觀念

***

### AI (Artificial intelligence)

- Machine Learning只是AI其中一個類別，Deep Learning只是Machine Learning其中的方法。
- 如果AI能夠完成人類所有智能工作，稱之為強AI (Strong AI) or 通用AI (Artificial General Intelligence)。
- AI中的理解圖像的領域，稱之為Computer Vision
- AI中的理解人類語言的領域，稱之為Natural Language Processing, NLP

### ML (Machine Learning)

- Machine Learning的目標是讓機器從訓練數據中自動進步，從經驗中自動學習。
- 除了Machine Learning外，AI還是有其他方法，例如 : 專家系統、博奕樹搜尋...。
- 相關研究議題及技術
    - Deep Learning
    - Support Vector Machine, SVM
    - Reinforcement Learning
        - 強化學習的目標是讓機器在環境中逐漸學會正確決策，最終獲得最大利益。 例如 : 自動玩遊戲。
    - Predictive Learning
        - 預測資料的後續發展，流行的作法也是使用Deep Learning

### DL (Deep Learning)

- 神經網路( Neural Network, NN ) 是由一層層的神經元 (neuron) 組成的。
- 傳統神經網路架構是使用 Feedforward Neural Network，其中包含 Multilayer Perceptron, MLP。
- Convolutional Neural Network, CNN是採用 convolution架構的神經網路。
- Recurrent Neural Network, RNN
    - Long-Short Term Memory, LSTM
    - Gated Recurrent Unit, GRU

***

### Deep Learning的一些基本想法
<br>

#### Fit
- 讓機器從訓練數據中自動進步，從經驗中自動學習，在這個過程中，就會出現Fit這個概念
- Fit指的是結果和訓練數據接近
- 可由通用逼近定理 ( universal approximation theorem )
- 但難以實際解決複雜問題，因為它容易出現over-fit
- 目前認識的是，Deep Neural Network的逐層結構可以實現對於概念的不斷抽象，這恰巧與世界的運行規律吻合。自然界中有這樣的抽象結構，人腦也有這樣的逐層抽象結構。

#### 自動發現規律 : 從數據A到答案B

- 我們不需要告訴它人類是如何判斷圖像中是狗還是貓，只需要給它足夠的圖像分類例子(成千上萬，越多越好)，他就能自動發現其中的規律。
- 傳統研究方法是輸入規則和數據，然後得到答案，但Learning方法是輸入答案和數據，然後找出規則

#### 從特徵工程到逐層抽象

- 早期圖像分類方法需要人工精心構造出許多特徵(feature)，稱為特徵工程。
- 用人臉特徵例子表示逐層抽象
    - 一個簡單的人臉辨識網路的可視化，第一層的神經元可以辨識簡單的線段邊界，第二層的神經元可辨識人臉的各個局部，第三層的神經元可以辨識相當完整的人臉。
- 對於許多複雜問題，網路越深，效果越好。
    - 如果將200的神經元拆成2層，每層100個神經元可以產生100*100 = 10000組合，比原來200個神經元表達能力更強。
    - 如果將200的神經元拆成4層，每層50個神經元可以產生50*50*50*50 = 6250000組合，表達能力更強。
- 不過，越複雜的網路，執行速度越慢。因為對於有速度要求的情況(例如圍棋程式)，網路架構須找到一個恰到好處的平衡點。

***

### 後言
而近10年，Computer Vision幾乎重要的領域及研究問題，幾乎都使用Deep Learning協助解決了，近幾年的CVPR Conference Papers也都跟Deep Learning相關。甚至Google Pixel中的大神Marc Levoy也根據相關理論研究並實作Pixel Camera中的Night Shot等功能，變成了實際產品應用在日常生活上。身為Computer Vision & Computer Graphics工作開發者，深入研究Deep Learning的發展，也逐漸變成非常重要的課題了。
